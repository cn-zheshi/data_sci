{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-da63cbe1",
   "language": "python",
   "display_name": "PyCharm (server)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 数据科学大作业后端\n",
    "\n",
    "爬取天涯、观察者网和微博的疫情相关文章的评论，分析疫情下的大众心理，并通过机器学习做出预测。\n",
    "\n",
    "## 项目进度\n",
    "\n",
    "- 12.16 创建flask项目框架 完成天涯爬虫（项目框架指一堆空文件夹；\n",
    "- 12.17 初步完成微博爬虫的爬取评论功能；\n",
    "\n",
    "## TODO\n",
    "\n",
    "- 12.13-12.20 完成项目框架的搭建，三个爬虫的整合，并开始进行数据分析；\n",
    "\n",
    "## 爬虫包接口\n",
    "\n",
    "`get_all()`返回一个列表，元素分别为文章/帖子/博文的主题、时间、url和评论，其中评论是列表形式。\n",
    "\n",
    "### usage\n",
    "\n",
    "调用时导入`util.spiders.<name>`中的`get_all()`函数即可。\n",
    "\n",
    "微博的爬虫如果想调整一次性爬取的结果数量，请手动调整`urls = find_urls(news_manager, <url>, <max_num>)`中的`max_num`。\n",
    "\n",
    "### config\n",
    "\n",
    "天涯的可以直接调用。\n",
    "\n",
    "微博需要手动配置。配置方法如下：\n",
    "\n",
    "- 安装`selenium`包，并进行浏览器驱动下载配置，具体请`STFW`；\n",
    "- 手动修改`util/news_parser.py`中`NewsManager`的`_dynamic`方法中的浏览器驱动路径`executable_path`；\n",
    "- 手动修改`util/spiders/weibo.py`中`get_all()`函数中的`name`和`passwd`，将其替换为你自己的微博账号密码，并且下载微博手机客户端保证能够扫码验证。\n",
    "\n",
    "微博的爬虫目前为单线程。\n",
    "\n",
    "以下为爬虫接口代码样例。"
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 微博爬虫样例\n",
    "from util.spiders.weibo import get_all\n",
    "\n",
    "final_res = get_all()\n",
    "\n",
    "for cur in final_res:\n",
    "    for comment in cur[-1]:\n",
    "        print(comment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}